{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f904acd",
   "metadata": {},
   "source": [
    "### 모델 학습용 코드 구현 및 실행\n",
    "\n",
    "- 학습별 코드 분리 (구분선 사용 및 해당 모델 이름 작성)\n",
    "- 학습된 파라미터는 ./parameters 에 .pth 형식으로 저장하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e61870",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = ['./data/2022Data_part1.csv', './data/2022Data_part2.csv']\n",
    "\n",
    "### Colab 사용시 주석 제거\n",
    "\n",
    "# !rm -rf SKN19_2ND_5TEAM\n",
    "# !git clone https://github.com/SKNetworks-AI19-250818/SKN19_2ND_5TEAM.git\n",
    "# %cd SKN19_2ND_5TEAM\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/SKN19_2ND_5TEAM')\n",
    "# input_file_path = ['/content/SKN19_2ND_5TEAM/data/encoded_dataset.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6121bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import modules.DataAnalysis as DataAnalysis\n",
    "import modules.ModelAnalysis as ModelAnalysis\n",
    "import modules.DataModify as DataModify\n",
    "from modules.DataSelect import DataPreprocessing\n",
    "\n",
    "import modules.Models as Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9614b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d425e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SKN_19\\SKN19_2ND_5TEAM\\modules\\DataModify.py:446: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 시드 고정 : 결과 비교용\n",
    "Models.set_seed(42)\n",
    "\n",
    "dp = DataPreprocessing()\n",
    "\n",
    "# device 설정 (cuda 사용 가능 시 cuda 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset 로드\n",
    "dataset = DataModify.CancerDataset(\n",
    "    target_column='target_label',              # target column\n",
    "    time_column='Survival months_bin_3m',      # Survival months\n",
    "    file_paths=input_file_path,\n",
    "    transform=dp.run                           # 기존에 정제가 완료된 데이터를 사용할 경우 None\n",
    ")\n",
    "\n",
    "sui_input_file_path = ['./data/Suicide.csv']\n",
    "sui_dataset = DataModify.CancerDataset(\n",
    "    target_column='target_label',              # target column\n",
    "    time_column='Survival months_bin_3m',      # Survival months\n",
    "    file_paths=sui_input_file_path,\n",
    "    transform=dp.run                           # 기존에 정제가 완료된 데이터를 사용할 경우 None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set size 설정 및 분리\n",
    "# 전체 길이\n",
    "n = len(dataset)\n",
    "\n",
    "# 비율 설정\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# 각 세트 크기 계산\n",
    "train_size = int(n * train_ratio)\n",
    "val_size = int(n * val_ratio)\n",
    "test_size = n - train_size - val_size  # 합이 정확히 맞도록 조정\n",
    "\n",
    "# 분리 수행\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_dataset = ConcatDataset([train_dataset, sui_dataset])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# 데이터를 로드\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = dataset.data.shape[1]   # input dimension : data의 feature의 개수\n",
    "hidden_size = (128, 64)             # 1번째, 2번째 hidden layer의 size\n",
    "time_bins = 91                      # 3개월 단위로 time을 split하여 각 구간으로 삼음 -> 270개월+ 는 하나로 취급\n",
    "num_events = 4                      # 사건의 개수\n",
    "\n",
    "# 모델 선언\n",
    "model = Models.DeepHitSurvWithSEBlockConcat(input_dim, hidden_size, time_bins, num_events, dropout=.2).to(device)\n",
    "\n",
    "# 손실함수 및 optimizer 선언\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52867e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "def train_epoch(model, loader, optimizer, device=device):\n",
    "    # 모델을 train 모드로 설정\n",
    "    model.train()\n",
    "    # loss 변수 선언\n",
    "    total_loss, total_lik, total_rank = 0, 0, 0\n",
    "\n",
    "    # loader에서 불러온 데이터를 기반으로 학습\n",
    "    for x, times, events in loader:\n",
    "        x, times, events = x.to(device), times.to(device), events.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, pmf, cif = model(x)\n",
    "        loss, L_lik, L_rank = Models.deephit_loss(pmf, cif, times, events)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_lik += L_lik.item() * x.size(0)\n",
    "        total_rank += L_rank.item() * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss/n, total_lik/n, total_rank/n\n",
    "\n",
    "# 모델 평가\n",
    "def evaluate(model, loader, device=device):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "    total_loss, total_lik, total_rank = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x, times, events = x.to(device), times.to(device), events.to(device)\n",
    "\n",
    "            logits, pmf, cif = model(x)\n",
    "            loss, L_lik, L_rank = Models.deephit_loss(pmf, cif, times, events)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_lik += L_lik.item() * x.size(0)\n",
    "            total_rank += L_rank.item() * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss/n, total_lik/n, total_rank/n\n",
    "\n",
    "def get_cif_from_model(model, loader, device=device):\n",
    "    model.eval()\n",
    "    all_cif = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x = x.to(device)\n",
    "            logits, pmf, cif = model(x)\n",
    "            all_cif.append(cif.cpu())\n",
    "            all_times.append(times)\n",
    "            all_events.append(events)\n",
    "    all_cif = torch.cat(all_cif, dim=0)  # (num_samples, num_events, time_bins)\n",
    "    all_times = torch.cat(all_times, dim=0)\n",
    "    all_events = torch.cat(all_events, dim=0)\n",
    "    return all_cif, all_times, all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce35d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] Train Loss=0.9030 (L=0.8940, R=0.0182) | Val Loss=0.7239 (L=0.7179, R=0.0119)\n",
      "[002] Train Loss=0.7527 (L=0.7450, R=0.0153) | Val Loss=0.6945 (L=0.6876, R=0.0139)\n",
      "[003] Train Loss=0.7387 (L=0.7312, R=0.0148) | Val Loss=0.6916 (L=0.6848, R=0.0135)\n",
      "[004] Train Loss=0.7323 (L=0.7250, R=0.0147) | Val Loss=0.6860 (L=0.6800, R=0.0121)\n",
      "[005] Train Loss=0.7294 (L=0.7221, R=0.0146) | Val Loss=0.6871 (L=0.6814, R=0.0113)\n",
      "[006] Train Loss=0.7269 (L=0.7196, R=0.0145) | Val Loss=0.6844 (L=0.6783, R=0.0123)\n",
      "[007] Train Loss=0.7256 (L=0.7183, R=0.0145) | Val Loss=0.6790 (L=0.6727, R=0.0126)\n",
      "[008] Train Loss=0.7248 (L=0.7175, R=0.0146) | Val Loss=0.6817 (L=0.6756, R=0.0122)\n",
      "[009] Train Loss=0.7233 (L=0.7161, R=0.0145) | Val Loss=0.6863 (L=0.6797, R=0.0132)\n",
      "[010] Train Loss=0.7225 (L=0.7153, R=0.0145) | Val Loss=0.6854 (L=0.6783, R=0.0143)\n",
      "[011] Train Loss=0.7221 (L=0.7149, R=0.0144) | Val Loss=0.6819 (L=0.6755, R=0.0128)\n",
      "[012] Train Loss=0.7220 (L=0.7147, R=0.0145) | Val Loss=0.6744 (L=0.6685, R=0.0119)\n",
      "[013] Train Loss=0.7212 (L=0.7140, R=0.0144) | Val Loss=0.6748 (L=0.6684, R=0.0127)\n",
      "[014] Train Loss=0.7207 (L=0.7135, R=0.0144) | Val Loss=0.6838 (L=0.6781, R=0.0115)\n",
      "[015] Train Loss=0.7204 (L=0.7131, R=0.0145) | Val Loss=0.6775 (L=0.6715, R=0.0120)\n",
      "[016] Train Loss=0.7208 (L=0.7135, R=0.0144) | Val Loss=0.6739 (L=0.6675, R=0.0128)\n",
      "[017] Train Loss=0.7216 (L=0.7143, R=0.0146) | Val Loss=0.6737 (L=0.6674, R=0.0127)\n",
      "[018] Train Loss=0.7208 (L=0.7135, R=0.0146) | Val Loss=0.6755 (L=0.6689, R=0.0132)\n",
      "[019] Train Loss=0.7210 (L=0.7137, R=0.0145) | Val Loss=0.6777 (L=0.6714, R=0.0125)\n",
      "[020] Train Loss=0.7210 (L=0.7138, R=0.0145) | Val Loss=0.6762 (L=0.6704, R=0.0116)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss, train_lik, train_rank = train_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_lik, val_rank = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"[{epoch:03d}] \"\n",
    "          f\"Train Loss={train_loss:.4f} (L={train_lik:.4f}, R={train_rank:.4f}) | \"\n",
    "          f\"Val Loss={val_loss:.4f} (L={val_lik:.4f}, R={val_rank:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba13781",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./data/parameters/deephit_model_feature_concat.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ed3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set CIF 추출\n",
    "cif_train, times_train, events_train = get_cif_from_model(model, train_loader)\n",
    "\n",
    "# 사건별 마지막 CIF를 입력으로 사용\n",
    "X_risk = cif_train[:, :, -2].numpy()  # (num_samples, num_events)\n",
    "weights = [0.2, 1, 0.3, 5]\n",
    "\n",
    "risk_target = np.zeros(X_risk.shape[0])\n",
    "for i in range(len(events_train)):\n",
    "    t_i = min(times_train[i], cif_train.shape[2]-2)  # 최대값 제한\n",
    "    if events_train[i] >= 0:\n",
    "        risk_target[i] = cif_train[i, events_train[i], t_i].item()\n",
    "    else:\n",
    "        risk_target[i] = cif_train[i, :, t_i].sum().item()  # 검열 처리\n",
    "\n",
    "risk_model = Models.WeightedCoxRiskEstimator(num_events=X_risk.shape[1], weights=weights, device=device)\n",
    "risk_model.fit(X_risk, times_train, events_train)\n",
    "\n",
    "torch.save(risk_model.event_linears.state_dict(), \"./data/parameters/risk_model_event_linears.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3468fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값: 99.712364\n",
      "최소값: 18.784138\n",
      "평균값: 77.81959\n",
      "앞 10개 값: [99.17208  67.95394  76.25765  98.17578  44.27909  58.175556 98.68389\n",
      " 98.74225  99.566154 96.214615]\n"
     ]
    }
   ],
   "source": [
    "risk_scores = risk_model.predict(X_risk)\n",
    "\n",
    "print(\"최대값:\", np.max(risk_scores))\n",
    "print(\"최소값:\", np.min(risk_scores))\n",
    "print(\"평균값:\", np.mean(risk_scores))\n",
    "print(\"앞 10개 값:\", risk_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 라벨별 Risk Score 통계 ===\n",
      "\n",
      "Event -1:\n",
      "  개수: 366123\n",
      "  최대값: 99.7118\n",
      "  최소값: 18.7841\n",
      "  평균값: 75.8396\n",
      "\n",
      "Event 0:\n",
      "  개수: 41868\n",
      "  최대값: 99.7124\n",
      "  최소값: 18.9264\n",
      "  평균값: 93.9902\n",
      "\n",
      "Event 1:\n",
      "  개수: 5857\n",
      "  최대값: 99.7050\n",
      "  최소값: 19.3771\n",
      "  평균값: 83.4820\n",
      "\n",
      "Event 2:\n",
      "  개수: 6164\n",
      "  최대값: 99.7020\n",
      "  최소값: 19.8268\n",
      "  평균값: 81.9528\n",
      "\n",
      "Event 3:\n",
      "  개수: 2441\n",
      "  최대값: 99.6347\n",
      "  최소값: 19.5633\n",
      "  평균값: 73.4187\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tensor → numpy 변환\n",
    "events_np = events_train.numpy()\n",
    "\n",
    "# 사건 라벨 종류 (-1은 검열)\n",
    "unique_events = np.unique(events_np)\n",
    "\n",
    "print(\"=== 라벨별 Risk Score 통계 ===\")\n",
    "for e in unique_events:\n",
    "    mask = (events_np == e)\n",
    "    scores_e = risk_scores[mask]\n",
    "\n",
    "    if len(scores_e) == 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEvent {e}:\")\n",
    "    print(f\"  개수: {len(scores_e)}\")\n",
    "    print(f\"  최대값: {np.max(scores_e):.4f}\")\n",
    "    print(f\"  최소값: {np.min(scores_e):.4f}\")\n",
    "    print(f\"  평균값: {np.mean(scores_e):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Team5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
