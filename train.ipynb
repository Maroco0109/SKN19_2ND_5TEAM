{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f904acd",
   "metadata": {},
   "source": [
    "### 모델 학습용 코드 구현 및 실행\n",
    "\n",
    "- 학습별 코드 분리 (구분선 사용 및 해당 모델 이름 작성)\n",
    "- 학습된 파라미터는 ./parameters 에 .pth 형식으로 저장하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e61870",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = ['./data/2022Data_part1.csv', './data/2022Data_part2.csv']\n",
    "\n",
    "### Colab 사용시 주석 제거\n",
    "\n",
    "# !rm -rf SKN19_2ND_5TEAM\n",
    "# !git clone https://github.com/SKNetworks-AI19-250818/SKN19_2ND_5TEAM.git\n",
    "# %cd SKN19_2ND_5TEAM\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/SKN19_2ND_5TEAM')\n",
    "# input_file_path = ['/content/SKN19_2ND_5TEAM/data/encoded_dataset.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6121bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader, ConcatDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import modules.DataAnalysis as DataAnalysis\n",
    "import modules.ModelAnalysis as ModelAnalysis\n",
    "import modules.DataModify as DataModify\n",
    "from modules.DataSelect import DataPreprocessing\n",
    "\n",
    "import modules.Models as Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9614b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d425e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SKN_19\\SKN19_2ND_5TEAM\\modules\\DataModify.py:501: DtypeWarning: Columns (24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path)\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 시드 고정 : 결과 비교용\n",
    "Models.set_seed(42)\n",
    "\n",
    "dp = DataPreprocessing()\n",
    "\n",
    "# device 설정 (cuda 사용 가능 시 cuda 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset 로드\n",
    "dataset = DataModify.CancerDataset(\n",
    "    target_column='target_label',              # target column\n",
    "    time_column='Survival months_bin_3m',      # Survival months\n",
    "    file_paths=input_file_path,\n",
    "    transform=dp.run                           # 기존에 정제가 완료된 데이터를 사용할 경우 None\n",
    ")\n",
    "\n",
    "sui_input_file_path = ['./data/Suicide.csv']\n",
    "sui_dataset = DataModify.CancerDataset(\n",
    "    target_column='target_label',              # target column\n",
    "    time_column='Survival months_bin_3m',      # Survival months\n",
    "    file_paths=sui_input_file_path,\n",
    "    transform=dp.run                           # 기존에 정제가 완료된 데이터를 사용할 경우 None\n",
    ")\n",
    "\n",
    "dp.save_category()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82b1b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set size 설정 및 분리\n",
    "# 전체 길이\n",
    "n = len(dataset)\n",
    "\n",
    "# 비율 설정\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# 각 세트 크기 계산\n",
    "train_size = int(n * train_ratio)\n",
    "val_size = int(n * val_ratio)\n",
    "test_size = n - train_size - val_size  # 합이 정확히 맞도록 조정\n",
    "\n",
    "# 분리 수행\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_dataset = ConcatDataset([train_dataset, sui_dataset])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# 데이터를 로드\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = dataset.data.shape[1]   # input dimension : data의 feature의 개수\n",
    "hidden_size = (128, 64)             # 1번째, 2번째 hidden layer의 size\n",
    "time_bins = 91                     # 3개월 단위로 time을 split하여 각 구간으로 삼음 -> 270개월+ 는 하나로 취급\n",
    "num_events = 4                      # 사건의 개수\n",
    "\n",
    "# 모델 선언\n",
    "model = Models.DeepHitSurvWithSEBlock(input_dim, hidden_size, time_bins, num_events, dropout=.2).to(device)\n",
    "\n",
    "# 손실함수 및 optimizer 선언\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccf2ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422453\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c767b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = ModelAnalysis.dataset_to_dataframe(test_dataset)\n",
    "\n",
    "df_save.to_csv(\"./data/test dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52867e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "def train_epoch(model, loader, optimizer, device=device):\n",
    "    # 모델을 train 모드로 설정\n",
    "    model.train()\n",
    "    # loss 변수 선언\n",
    "    total_loss, total_lik, total_rank = 0, 0, 0\n",
    "\n",
    "    # loader에서 불러온 데이터를 기반으로 학습\n",
    "    for x, times, events in loader:\n",
    "        x, times, events = x.to(device), times.to(device), events.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, pmf, cif = model(x)\n",
    "        loss, L_lik, L_rank = Models.deephit_loss(pmf, cif, times, events)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_lik += L_lik.item() * x.size(0)\n",
    "        total_rank += L_rank.item() * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss/n, total_lik/n, total_rank/n\n",
    "\n",
    "# 모델 평가\n",
    "def evaluate(model, loader, device=device):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "    total_loss, total_lik, total_rank = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x, times, events = x.to(device), times.to(device), events.to(device)\n",
    "\n",
    "            logits, pmf, cif = model(x)\n",
    "            loss, L_lik, L_rank = Models.deephit_loss(pmf, cif, times, events)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_lik += L_lik.item() * x.size(0)\n",
    "            total_rank += L_rank.item() * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss/n, total_lik/n, total_rank/n\n",
    "\n",
    "def get_cif_from_model(model, loader, device=device):\n",
    "    model.eval()\n",
    "    all_cif = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x = x.to(device)\n",
    "            logits, pmf, cif = model(x)\n",
    "            all_cif.append(cif.cpu())\n",
    "            all_times.append(times)\n",
    "            all_events.append(events)\n",
    "    all_cif = torch.cat(all_cif, dim=0)  # (num_samples, num_events, time_bins)\n",
    "    all_times = torch.cat(all_times, dim=0)\n",
    "    all_events = torch.cat(all_events, dim=0)\n",
    "    return all_cif, all_times, all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fce35d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] Train Loss=0.8033 (L=0.7942, R=0.0182) | Val Loss=0.7323 (L=0.7256, R=0.0134)\n",
      "[002] Train Loss=0.7919 (L=0.7837, R=0.0164) | Val Loss=0.7408 (L=0.7331, R=0.0152)\n",
      "[003] Train Loss=0.7774 (L=0.7697, R=0.0154) | Val Loss=0.7150 (L=0.7078, R=0.0143)\n",
      "[004] Train Loss=0.7662 (L=0.7587, R=0.0150) | Val Loss=0.7044 (L=0.6984, R=0.0119)\n",
      "[005] Train Loss=0.7623 (L=0.7549, R=0.0149) | Val Loss=0.7064 (L=0.7009, R=0.0110)\n",
      "[006] Train Loss=0.7588 (L=0.7514, R=0.0148) | Val Loss=0.7040 (L=0.6982, R=0.0116)\n",
      "[007] Train Loss=0.7569 (L=0.7495, R=0.0149) | Val Loss=0.6959 (L=0.6895, R=0.0129)\n",
      "[008] Train Loss=0.7559 (L=0.7484, R=0.0149) | Val Loss=0.7007 (L=0.6939, R=0.0136)\n",
      "[009] Train Loss=0.7540 (L=0.7465, R=0.0149) | Val Loss=0.6982 (L=0.6923, R=0.0118)\n",
      "[010] Train Loss=0.7531 (L=0.7458, R=0.0148) | Val Loss=0.7035 (L=0.6964, R=0.0142)\n",
      "[011] Train Loss=0.7529 (L=0.7455, R=0.0148) | Val Loss=0.7019 (L=0.6958, R=0.0123)\n",
      "[012] Train Loss=0.7518 (L=0.7445, R=0.0147) | Val Loss=0.6972 (L=0.6911, R=0.0121)\n",
      "[013] Train Loss=0.7517 (L=0.7443, R=0.0147) | Val Loss=0.6961 (L=0.6900, R=0.0123)\n",
      "[014] Train Loss=0.7514 (L=0.7440, R=0.0147) | Val Loss=0.7074 (L=0.7012, R=0.0124)\n",
      "[015] Train Loss=0.7504 (L=0.7431, R=0.0147) | Val Loss=0.6961 (L=0.6901, R=0.0119)\n",
      "[016] Train Loss=0.7506 (L=0.7432, R=0.0147) | Val Loss=0.6928 (L=0.6866, R=0.0122)\n",
      "[017] Train Loss=0.7510 (L=0.7436, R=0.0149) | Val Loss=0.6912 (L=0.6849, R=0.0125)\n",
      "[018] Train Loss=0.7502 (L=0.7429, R=0.0148) | Val Loss=0.6939 (L=0.6879, R=0.0121)\n",
      "[019] Train Loss=0.7508 (L=0.7434, R=0.0147) | Val Loss=0.6969 (L=0.6903, R=0.0132)\n",
      "[020] Train Loss=0.7514 (L=0.7441, R=0.0147) | Val Loss=0.7001 (L=0.6939, R=0.0124)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss, train_lik, train_rank = train_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_lik, val_rank = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"[{epoch:03d}] \"\n",
    "          f\"Train Loss={train_loss:.4f} (L={train_lik:.4f}, R={train_rank:.4f}) | \"\n",
    "          f\"Val Loss={val_loss:.4f} (L={val_lik:.4f}, R={val_rank:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba13781",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./parameters/deephit_model_feature.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44512d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepHitSurvWithSEBlock(\n",
       "  (se_block): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=4, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=4, out_features=17, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (se_block_event): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=64, out_features=16, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=16, out_features=64, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (shared): Sequential(\n",
       "    (0): Linear(in_features=17, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (heads): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=64, out_features=91, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_params_path = './parameters/deephit_model_feature.pth'\n",
    "\n",
    "input_dim = dataset.data.shape[1]   # input dimension : data의 feature의 개수\n",
    "hidden_size = (128, 64)             # 1번째, 2번째 hidden layer의 size\n",
    "time_bins = 91                      # 3개월 단위로 time을 split하여 각 구간으로 삼음 -> 최대 270개월 + 그 후\n",
    "num_events = 4                      # 사건의 개수\n",
    "\n",
    "# 모델 정의 (학습할 때 사용한 모델 클래스)\n",
    "model = Models.DeepHitSurvWithSEBlock(input_dim, \n",
    "                    hidden_size, \n",
    "                    time_bins, \n",
    "                    num_events,\n",
    "                    )  # 사건 수 맞게 설정\n",
    "model.load_state_dict(torch.load(input_params_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # 평가 모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f5407",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# train set CIF 추출\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cif_train, times_train, events_train \u001b[38;5;241m=\u001b[39m \u001b[43mget_cif_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 53\u001b[0m, in \u001b[0;36mget_cif_from_model\u001b[1;34m(model, loader, device)\u001b[0m\n\u001b[0;32m     51\u001b[0m all_events \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, times, events \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     54\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     55\u001b[0m         logits, pmf, cif \u001b[38;5;241m=\u001b[39m model(x)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Team5\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train set CIF 추출\n",
    "cif_train, times_train, events_train = get_cif_from_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d510be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_score_sigmoid(pmf, time_lambda=0.05, event_weights=None):\n",
    "    \"\"\"\n",
    "    pmf: torch.Tensor, shape (B, E, T) - 사건별 시간 확률\n",
    "    time_lambda: float, 지수 감쇠 계수 (시간대 가중치)\n",
    "    event_weights: list or torch.Tensor, 길이 E, 사건별 가중치\n",
    "    \"\"\"\n",
    "    B, E, T = pmf.shape\n",
    "    device = pmf.device\n",
    "\n",
    "    # 시간 가중치\n",
    "    time_weights = torch.exp(-time_lambda * torch.arange(T, device=device))\n",
    "    \n",
    "    # 사건 가중치\n",
    "    if event_weights is None:\n",
    "        event_weights = torch.ones(E, device=device)\n",
    "    else:\n",
    "        event_weights = torch.tensor(event_weights, device=device, dtype=torch.float32)\n",
    "    \n",
    "    # 가중치 적용\n",
    "    weighted_pmf = pmf * time_weights.view(1, 1, T)\n",
    "    weighted_pmf = weighted_pmf * event_weights.view(1, E, 1)\n",
    "\n",
    "    # 가중합 계산\n",
    "    risk_score_raw = weighted_pmf.sum(dim=(1, 2))\n",
    "\n",
    "    # 0 기준으로 offset 제거 → 음수도 나오게\n",
    "    risk_score_raw = risk_score_raw - risk_score_raw.mean()\n",
    "\n",
    "    # 시그모이드 + 0~100 스케일\n",
    "    risk_score = torch.sigmoid(risk_score_raw) * 100\n",
    "\n",
    "    return risk_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48265bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에서 PMF 추출\n",
    "def get_pmf_from_model(model, loader, device=device):\n",
    "    model.eval()\n",
    "    all_pmf = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x = x.to(device)\n",
    "            logits, pmf, _ = model(x)  # CIF는 필요 없음\n",
    "\n",
    "            pmf = pmf[:, :, :91]  # (batch_size, num_events, time_bins-1)\n",
    "            \n",
    "            all_pmf.append(pmf.cpu())\n",
    "            all_times.append(times)\n",
    "            all_events.append(events)\n",
    "    all_pmf = torch.cat(all_pmf, dim=0)  # (num_samples, num_events, time_bins)\n",
    "    all_times = torch.cat(all_times, dim=0)\n",
    "    all_events = torch.cat(all_events, dim=0)\n",
    "    return all_pmf, all_times, all_events\n",
    " \n",
    "# train set PMF 추출\n",
    "pmf_train, times_train, events_train = get_pmf_from_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f147eb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값: 99.41755\n",
      "최소값: 38.345165\n",
      "평균값: 49.81617\n",
      "앞 10개 값: [41.35229  39.432365 39.459236 45.144897 53.892452 60.673405 40.995117\n",
      " 43.67246  41.11936  47.19217 ]\n",
      "=== 라벨별 Risk Score 통계 ===\n",
      "\n",
      "Event -1:\n",
      "  개수: 366123\n",
      "  최대값: 98.9653\n",
      "  최소값: 38.3452\n",
      "  평균값: 48.3918\n",
      "\n",
      "Event 0:\n",
      "  개수: 41868\n",
      "  최대값: 98.0343\n",
      "  최소값: 38.4692\n",
      "  평균값: 61.6722\n",
      "\n",
      "Event 1:\n",
      "  개수: 5857\n",
      "  최대값: 98.0431\n",
      "  최소값: 38.4692\n",
      "  평균값: 52.0595\n",
      "\n",
      "Event 2:\n",
      "  개수: 6164\n",
      "  최대값: 97.9004\n",
      "  최소값: 38.4692\n",
      "  평균값: 51.0170\n",
      "\n",
      "Event 3:\n",
      "  개수: 2441\n",
      "  최대값: 99.4175\n",
      "  최소값: 38.4692\n",
      "  평균값: 51.6823\n"
     ]
    }
   ],
   "source": [
    "# 사건별 가중치 설정\n",
    "event_weights = [2.0, 3.0, 3.0, 7.0]  # 예시\n",
    "\n",
    "# 위험 점수 계산 (시그모이드 + 0~100)\n",
    "risk_scores = compute_risk_score_sigmoid(pmf_train, time_lambda=0.05, event_weights=event_weights).numpy()\n",
    "\n",
    "# 통계 확인\n",
    "print(\"최대값:\", np.max(risk_scores))\n",
    "print(\"최소값:\", np.min(risk_scores))\n",
    "print(\"평균값:\", np.mean(risk_scores))\n",
    "print(\"앞 10개 값:\", risk_scores[:10])\n",
    "\n",
    "# 사건별 통계\n",
    "events_np = events_train.numpy()\n",
    "unique_events = np.unique(events_np)\n",
    "\n",
    "print(\"=== 라벨별 Risk Score 통계 ===\")\n",
    "for e in unique_events:\n",
    "    mask = (events_np == e)\n",
    "    scores_e = risk_scores[mask]\n",
    "    if len(scores_e) == 0:\n",
    "        continue\n",
    "    print(f\"\\nEvent {e}:\")\n",
    "    print(f\"  개수: {len(scores_e)}\")\n",
    "    print(f\"  최대값: {np.max(scores_e):.4f}\")\n",
    "    print(f\"  최소값: {np.min(scores_e):.4f}\")\n",
    "    print(f\"  평균값: {np.mean(scores_e):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ed3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SKN_19\\SKN19_2ND_5TEAM\\modules\\Models.py:501: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  times = torch.tensor(times, dtype=torch.float32, device=self.device)\n",
      "d:\\SKN_19\\SKN19_2ND_5TEAM\\modules\\Models.py:502: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  events = torch.tensor(events, dtype=torch.float32, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "# 사건별 마지막 CIF를 입력으로 사용\n",
    "X_risk = cif_train[:, :, -2].numpy()  # (num_samples, num_events)\n",
    "weights = [0.3, 0.3, 1, 3]\n",
    "\n",
    "risk_target = np.zeros(X_risk.shape[0])\n",
    "for i in range(len(events_train)):\n",
    "    t_i = min(times_train[i], cif_train.shape[2]-2)  # 최대값 제한\n",
    "    if events_train[i] >= 0:\n",
    "        risk_target[i] = cif_train[i, events_train[i], t_i].item()\n",
    "    else:\n",
    "        risk_target[i] = cif_train[i, :, t_i].sum().item()  # 검열 처리\n",
    "\n",
    "risk_model = Models.WeightedCoxRiskEstimator(num_events=X_risk.shape[1], weights=weights, device=device)\n",
    "risk_model.fit(X_risk, times_train, events_train)\n",
    "\n",
    "torch.save(risk_model.event_linears.state_dict(), \"./data/parameters/risk_model_event_linears.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3468fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값: 9.331824\n",
      "최소값: 0.010618577\n",
      "평균값: 1.2780025\n",
      "앞 10개 값: [3.1551757  3.731631   0.01266463 4.1761475  4.428556   2.6529982\n",
      " 2.1935594  0.08597407 0.02723856 0.2810931 ]\n"
     ]
    }
   ],
   "source": [
    "risk_scores = risk_model.predict(X_risk)\n",
    "\n",
    "print(\"최대값:\", np.max(risk_scores))\n",
    "print(\"최소값:\", np.min(risk_scores))\n",
    "print(\"평균값:\", np.mean(risk_scores))\n",
    "print(\"앞 10개 값:\", risk_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 라벨별 Risk Score 통계 ===\n",
      "\n",
      "Event -1:\n",
      "  개수: 366123\n",
      "  최대값: 9.3318\n",
      "  최소값: 0.0106\n",
      "  평균값: 1.2798\n",
      "\n",
      "Event 0:\n",
      "  개수: 41868\n",
      "  최대값: 9.3318\n",
      "  최소값: 0.0108\n",
      "  평균값: 1.2656\n",
      "\n",
      "Event 1:\n",
      "  개수: 5857\n",
      "  최대값: 9.3318\n",
      "  최소값: 0.0107\n",
      "  평균값: 1.2517\n",
      "\n",
      "Event 2:\n",
      "  개수: 6164\n",
      "  최대값: 9.3318\n",
      "  최소값: 0.0110\n",
      "  평균값: 1.2711\n",
      "\n",
      "Event 3:\n",
      "  개수: 2441\n",
      "  최대값: 9.3265\n",
      "  최소값: 0.0111\n",
      "  평균값: 1.3035\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tensor → numpy 변환\n",
    "events_np = events_train.numpy()\n",
    "\n",
    "# 사건 라벨 종류 (-1은 검열)\n",
    "unique_events = np.unique(events_np)\n",
    "\n",
    "print(\"=== 라벨별 Risk Score 통계 ===\")\n",
    "for e in unique_events:\n",
    "    mask = (events_np == e)\n",
    "    scores_e = risk_scores[mask]\n",
    "\n",
    "    if len(scores_e) == 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEvent {e}:\")\n",
    "    print(f\"  개수: {len(scores_e)}\")\n",
    "    print(f\"  최대값: {np.max(scores_e):.4f}\")\n",
    "    print(f\"  최소값: {np.min(scores_e):.4f}\")\n",
    "    print(f\"  평균값: {np.mean(scores_e):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Team5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
