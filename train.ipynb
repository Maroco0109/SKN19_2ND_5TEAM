{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f904acd",
   "metadata": {},
   "source": [
    "### 모델 학습용 코드 구현 및 실행\n",
    "\n",
    "- 학습별 코드 분리 (구분선 사용 및 해당 모델 이름 작성)\n",
    "- 학습된 파라미터는 ./parameters 에 .pth 형식으로 저장하여 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e61870",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = ['./data/encoded_dataset.csv']\n",
    "\n",
    "### Colab 사용시 주석 제거\n",
    "\n",
    "# !rm -rf SKN19_2ND_5TEAM\n",
    "# !git clone https://github.com/SKNetworks-AI19-250818/SKN19_2ND_5TEAM.git\n",
    "# %cd SKN19_2ND_5TEAM\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/SKN19_2ND_5TEAM')\n",
    "# input_file_path = ['/content/SKN19_2ND_5TEAM/data/encoded_dataset.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6121bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "import modules.DataAnalysis as DataAnalysis\n",
    "import modules.ModelAnalysis as ModelAnalysis\n",
    "import modules.DataModify as DataModify\n",
    "from modules.DataModify import DataPreprocessing\n",
    "\n",
    "import modules.Models as Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9614b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu126\n",
      "12.6\n",
      "91002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528d36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Suicide2010-2021.csv\")\n",
    "dm = DataModify.DataPreprocessing(df)\n",
    "df_sui,_,_ = dm.run()\n",
    "df_sui.to_csv(\"./data/Suicide_encode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82b1b6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 시드 고정 : 결과 비교용\n",
    "Models.set_seed(42)\n",
    "\n",
    "# device 설정 (cuda 사용 가능 시 cuda 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset 로드\n",
    "dataset = DataModify.CancerDataset(\n",
    "    target_column='target_label',              # target column\n",
    "    time_column='Survival months_bin_3m',      # Survival months\n",
    "    file_paths=input_file_path,\n",
    "    transform=None          # 기존에 정제가 완료된 데이터를 사용할 경우 None\n",
    ")\n",
    "\n",
    "\n",
    "# train set size 설정 및 분리\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# 데이터를 로드\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = dataset.data.shape[1]   # input dimension : data의 feature의 개수\n",
    "hidden_size = (128, 64)             # 1번째, 2번째 hidden layer의 size\n",
    "time_bins = 91                      # 3개월 단위로 time을 split하여 각 구간으로 삼음 -> 270개월+ 는 하나로 취급\n",
    "num_events = 4                      # 사건의 개수\n",
    "\n",
    "# 모델 선언\n",
    "model = Models.DeepHitSurvWithSEBlock(input_dim, hidden_size, time_bins, num_events, dropout=.2).to(device)\n",
    "\n",
    "# 손실함수 및 optimizer 선언\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52867e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "def train_epoch(model, loader, optimizer, device=device):\n",
    "    # 모델을 train 모드로 설정\n",
    "    model.train()\n",
    "    # loss 변수 선언\n",
    "    total_loss, total_lik, total_rank = 0, 0, 0\n",
    "\n",
    "    # loader에서 불러온 데이터를 기반으로 학습\n",
    "    for x, times, events in loader:\n",
    "        x, times, events = x.to(device), times.to(device), events.to(device)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, pmf, cif = model(x)\n",
    "        loss, L_lik, L_rank = Models.deephit_loss(pmf, cif, times, events)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_lik += L_lik.item() * x.size(0)\n",
    "        total_rank += L_rank.item() * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss/n, total_lik/n, total_rank/n\n",
    "\n",
    "# 모델 평가\n",
    "def evaluate(model, loader, device=device):\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "    total_loss, total_lik, total_rank = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x, times, events = x.to(device), times.to(device), events.to(device)\n",
    "\n",
    "            logits, pmf, cif = model(x)\n",
    "            loss, L_lik, L_rank = Models.deephit_loss(pmf, cif, times, events)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_lik += L_lik.item() * x.size(0)\n",
    "            total_rank += L_rank.item() * x.size(0)\n",
    "\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss/n, total_lik/n, total_rank/n\n",
    "\n",
    "def get_cif_from_model(model, loader, device=device):\n",
    "    model.eval()\n",
    "    all_cif = []\n",
    "    all_times = []\n",
    "    all_events = []\n",
    "    with torch.no_grad():\n",
    "        for x, times, events in loader:\n",
    "            x = x.to(device)\n",
    "            logits, pmf, cif = model(x)\n",
    "            all_cif.append(cif.cpu())\n",
    "            all_times.append(times)\n",
    "            all_events.append(events)\n",
    "    all_cif = torch.cat(all_cif, dim=0)  # (num_samples, num_events, time_bins)\n",
    "    all_times = torch.cat(all_times, dim=0)\n",
    "    all_events = torch.cat(all_events, dim=0)\n",
    "    return all_cif, all_times, all_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fce35d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] Train Loss=1.1529 (L=1.1394, R=0.0271) | Val Loss=0.8056 (L=0.7959, R=0.0193)\n",
      "[002] Train Loss=0.7883 (L=0.7791, R=0.0184) | Val Loss=0.7620 (L=0.7519, R=0.0202)\n",
      "[003] Train Loss=0.7551 (L=0.7463, R=0.0176) | Val Loss=0.7274 (L=0.7194, R=0.0160)\n",
      "[004] Train Loss=0.7470 (L=0.7384, R=0.0172) | Val Loss=0.7271 (L=0.7193, R=0.0156)\n",
      "[005] Train Loss=0.7436 (L=0.7351, R=0.0169) | Val Loss=0.7256 (L=0.7173, R=0.0167)\n",
      "[006] Train Loss=0.7411 (L=0.7326, R=0.0169) | Val Loss=0.7394 (L=0.7295, R=0.0198)\n",
      "[007] Train Loss=0.7394 (L=0.7310, R=0.0168) | Val Loss=0.7228 (L=0.7156, R=0.0143)\n",
      "[008] Train Loss=0.7386 (L=0.7302, R=0.0169) | Val Loss=0.7216 (L=0.7146, R=0.0140)\n",
      "[009] Train Loss=0.7370 (L=0.7286, R=0.0168) | Val Loss=0.7229 (L=0.7141, R=0.0176)\n",
      "[010] Train Loss=0.7367 (L=0.7283, R=0.0168) | Val Loss=0.7183 (L=0.7110, R=0.0145)\n",
      "[011] Train Loss=0.7353 (L=0.7270, R=0.0168) | Val Loss=0.7257 (L=0.7174, R=0.0165)\n",
      "[012] Train Loss=0.7351 (L=0.7268, R=0.0167) | Val Loss=0.7188 (L=0.7121, R=0.0135)\n",
      "[013] Train Loss=0.7367 (L=0.7284, R=0.0166) | Val Loss=0.7226 (L=0.7149, R=0.0154)\n",
      "[014] Train Loss=0.7351 (L=0.7268, R=0.0167) | Val Loss=0.7194 (L=0.7116, R=0.0157)\n",
      "[015] Train Loss=0.7356 (L=0.7272, R=0.0167) | Val Loss=0.7135 (L=0.7053, R=0.0164)\n",
      "[016] Train Loss=0.7347 (L=0.7264, R=0.0166) | Val Loss=0.7174 (L=0.7091, R=0.0164)\n",
      "[017] Train Loss=0.7344 (L=0.7260, R=0.0167) | Val Loss=0.7205 (L=0.7121, R=0.0168)\n",
      "[018] Train Loss=0.7365 (L=0.7281, R=0.0167) | Val Loss=0.7297 (L=0.7212, R=0.0170)\n",
      "[019] Train Loss=0.7365 (L=0.7281, R=0.0168) | Val Loss=0.7186 (L=0.7105, R=0.0162)\n",
      "[020] Train Loss=0.7381 (L=0.7297, R=0.0168) | Val Loss=0.7271 (L=0.7184, R=0.0174)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss, train_lik, train_rank = train_epoch(model, train_loader, optimizer)\n",
    "    val_loss, val_lik, val_rank = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"[{epoch:03d}] \"\n",
    "          f\"Train Loss={train_loss:.4f} (L={train_lik:.4f}, R={train_rank:.4f}) | \"\n",
    "          f\"Val Loss={val_loss:.4f} (L={val_lik:.4f}, R={val_rank:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eba13781",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./data/parameters/deephit_model_without_drop_cols.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21089567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class WeightedCoxRiskEstimator(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, num_events=4, lr=1e-2, epochs=100, weights=None, verbose=False, device='cpu'):\n",
    "        self.num_events = num_events\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        \n",
    "        # 가중치는 학습에 사용되지 않음\n",
    "        if weights is None:\n",
    "            self.weights = torch.ones(num_events, device=device) / num_events\n",
    "        else:\n",
    "            self.weights = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "\n",
    "    def _cox_ph_loss(self, risk_score, times, events):\n",
    "        risk_score = risk_score.squeeze()\n",
    "        loss = 0.0\n",
    "        uncensored_idx = (events >= 0).nonzero()[0]\n",
    "        if len(uncensored_idx) == 0:\n",
    "            return torch.tensor(0.0, device=self.device)\n",
    "        for i in uncensored_idx:\n",
    "            t_i = times[i]\n",
    "            mask = times >= t_i\n",
    "            loss += - (risk_score[i] - torch.log(torch.exp(risk_score[mask]).sum()))\n",
    "        return loss / len(uncensored_idx)\n",
    "\n",
    "    def fit(self, X, times, events):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        times = torch.tensor(times, dtype=torch.float32, device=self.device)\n",
    "        events = torch.tensor(events, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        B, K = X.shape\n",
    "        assert K == self.num_events\n",
    "\n",
    "        # 사건별 단층 선형 회귀(SLP)\n",
    "        self.event_linears = nn.ModuleList([nn.Linear(1, 1) for _ in range(K)]).to(self.device)\n",
    "\n",
    "        optimizer = optim.Adam(self.event_linears.parameters(), lr=self.lr)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 사건별 위험점수\n",
    "            r_list = [self.event_linears[k](X[:, k:k+1]) for k in range(K)]\n",
    "            r_stack = torch.cat(r_list, dim=1)  # (B, K)\n",
    "\n",
    "            # 학습 시에는 단순 평균 (weights 미적용)\n",
    "            risk_score = r_stack.mean(dim=1)\n",
    "\n",
    "            # Cox loss 계산\n",
    "            loss = self._cox_ph_loss(risk_score, times, events)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if self.verbose and (epoch % 10 == 0 or epoch == self.epochs - 1):\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        r_list = [self.event_linears[k](X[:, k:k+1]) for k in range(self.num_events)]\n",
    "        r_stack = torch.cat(r_list, dim=1)\n",
    "\n",
    "        # 예측 시에만 가중치 적용\n",
    "        risk_score = (r_stack * self.weights).sum(dim=1)\n",
    "\n",
    "        # Sigmoid 적용 후 0~100 스케일\n",
    "        risk_score_scaled = torch.sigmoid(risk_score) * 100\n",
    "\n",
    "        return risk_score_scaled.detach().cpu().numpy()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63ed3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19840\\846189422.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  times = torch.tensor(times, dtype=torch.float32, device=self.device)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19840\\846189422.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  events = torch.tensor(events, dtype=torch.float32, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "# train set CIF 추출\n",
    "cif_train, times_train, events_train = get_cif_from_model(model, train_loader)\n",
    "\n",
    "# 사건별 마지막 CIF를 입력으로 사용\n",
    "X_risk = cif_train[:, :, -2].numpy()  # (num_samples, num_events)\n",
    "weights = [1.5, 3, 0.5, 3]\n",
    "\n",
    "risk_target = np.zeros(X_risk.shape[0])\n",
    "for i in range(len(events_train)):\n",
    "    t_i = min(times_train[i], cif_train.shape[2]-2)  # 최대값 제한\n",
    "    if events_train[i] >= 0:\n",
    "        risk_target[i] = cif_train[i, events_train[i], t_i].item()\n",
    "    else:\n",
    "        risk_target[i] = cif_train[i, :, t_i].sum().item()  # 검열 처리\n",
    "\n",
    "risk_model = WeightedCoxRiskEstimator(num_events=X_risk.shape[1], weights=weights, device=device)\n",
    "risk_model.fit(X_risk, times_train, events_train)\n",
    "\n",
    "torch.save(risk_model.event_linears.state_dict(), \"./data/parameters/risk_model_event_linears.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ad3468fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대값: 91.94423\n",
      "최소값: 5.954586\n",
      "평균값: 68.19512\n",
      "앞 10개 값: [ 7.1735663 18.823639  24.198622  12.140998  54.68644    7.11794\n",
      " 15.090263  52.445255  90.72409   91.4985   ]\n"
     ]
    }
   ],
   "source": [
    "risk_scores = risk_model.predict(X_risk)\n",
    "\n",
    "print(\"최대값:\", np.max(risk_scores))\n",
    "print(\"최소값:\", np.min(risk_scores))\n",
    "print(\"평균값:\", np.mean(risk_scores))\n",
    "print(\"앞 10개 값:\", risk_scores[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45b7562c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 라벨별 Risk Score 통계 ===\n",
      "\n",
      "Event -1:\n",
      "  개수: 366514\n",
      "  최대값: 91.9442\n",
      "  최소값: 5.9546\n",
      "  평균값: 66.4709\n",
      "\n",
      "Event 0:\n",
      "  개수: 42018\n",
      "  최대값: 91.8404\n",
      "  최소값: 6.5019\n",
      "  평균값: 84.3570\n",
      "\n",
      "Event 1:\n",
      "  개수: 5800\n",
      "  최대값: 91.8333\n",
      "  최소값: 6.4398\n",
      "  평균값: 66.2527\n",
      "\n",
      "Event 2:\n",
      "  개수: 6140\n",
      "  최대값: 91.8419\n",
      "  최소값: 6.4507\n",
      "  평균값: 62.3290\n",
      "\n",
      "Event 3:\n",
      "  개수: 95\n",
      "  최대값: 91.7119\n",
      "  최소값: 6.8284\n",
      "  평균값: 69.6589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tensor → numpy 변환\n",
    "events_np = events_train.numpy()\n",
    "\n",
    "# 사건 라벨 종류 (-1은 검열)\n",
    "unique_events = np.unique(events_np)\n",
    "\n",
    "print(\"=== 라벨별 Risk Score 통계 ===\")\n",
    "for e in unique_events:\n",
    "    mask = (events_np == e)\n",
    "    scores_e = risk_scores[mask]\n",
    "\n",
    "    if len(scores_e) == 0:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nEvent {e}:\")\n",
    "    print(f\"  개수: {len(scores_e)}\")\n",
    "    print(f\"  최대값: {np.max(scores_e):.4f}\")\n",
    "    print(f\"  최소값: {np.min(scores_e):.4f}\")\n",
    "    print(f\"  평균값: {np.mean(scores_e):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Team5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
